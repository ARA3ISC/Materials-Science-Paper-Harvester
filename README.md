# ğŸ”¬ Materials Science Papers Harvester

A Streamlit web app to **search, aggregate, and download scientific papers in materials science**.
It queries multiple popular sources (OpenAlex, Crossref, arXiv, Semantic Scholar, DOAJ, PubMed, Springer, Elsevier/ScienceDirect, IEEE Xplore), deduplicates results, enriches with Unpaywall and landing-page scraping to recover missing PDF URLs, and exports clean JSONL/CSV files. Users can preview results and **download all available PDFs as a single ZIP** to their local computer.

---

## âœ¨ Features

* Search by **topic keywords** and year range.
* Aggregate results from many literature APIs.
* Normalize records into a consistent schema.
* Deduplicate using DOI and fuzzy title matching.
* Enrich missing PDF URLs via Unpaywall + landing-page scraping.
* Export to **CSV** and **JSONL**.
* **Streamlit UI**:

  * Logs and progress indicators.
  * Preview CSV in-browser.
  * One-click **download of all PDFs as a ZIP**.
  * Download failures log (for missing/broken PDFs).

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py                        # Streamlit UI
â”œâ”€â”€ materials_papers_harvester.py # Main harvester (no PDF download)
â”œâ”€â”€ download_verified_pdfs.py     # Bulk PDF downloader + verification
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Ignore venv + outputs
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml              # Local secrets (API keys, optional)
â””â”€â”€ runs/                         # Created at runtime, holds outputs
```

---

## âš™ï¸ Requirements

* Python 3.9+
* Packages listed in `requirements.txt`:

  ```
  streamlit
  pandas
  requests
  beautifulsoup4
  tenacity
  rapidfuzz
  pypdf
  ```

---

## ğŸš€ Running Locally

1. Clone the repo:

   ```bash
   git clone https://github.com/your-username/materials-harvester.git
   cd materials-harvester
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate     # Windows: venv\Scripts\activate
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Add your API keys (see below) in `.streamlit/secrets.toml`:

   ```toml
   SPRINGER_API_KEY = "your-springer-meta-key"
   ELSEVIER_API_KEY = "your-elsevier-key"
   IEEE_API_KEY = "your-ieee-key"
   CROSSREF_EMAIL = "your.email@domain.com"
   EMAIL = "your.email@domain.com"
   SEMANTIC_SCHOLAR_API_KEY = "optional-semantic-key"
   ```

5. Run the app:

   ```bash
   streamlit run app.py
   ```

Open [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ”‘ API Keys / Secrets

* **Springer Metadata API** â†’ `SPRINGER_API_KEY` (use Meta API key)
* **Elsevier (ScienceDirect) API** â†’ `ELSEVIER_API_KEY`
* **IEEE Xplore API** â†’ `IEEE_API_KEY`
* **Crossref & PubMed** â†’ set your email for `CROSSREF_EMAIL` and `EMAIL`
* **Semantic Scholar API** (optional) â†’ `SEMANTIC_SCHOLAR_API_KEY`

### On Streamlit Cloud

Set secrets in the app settings (App page â†’ â‹® â†’ Settings â†’ Secrets). Paste JSON like:

```json
{
  "SPRINGER_API_KEY": "xxxxx",
  "ELSEVIER_API_KEY": "xxxxx",
  "IEEE_API_KEY": "xxxxx",
  "CROSSREF_EMAIL": "you@domain.com",
  "EMAIL": "you@domain.com",
  "SEMANTIC_SCHOLAR_API_KEY": "optional"
}
```

---

## â˜ï¸ Deploying on Streamlit Cloud

1. Push this repo to GitHub.
2. Go to [https://share.streamlit.io](https://share.streamlit.io) and click **New app**.
3. Select your repo/branch and set `app.py` as the entrypoint.
4. Add API keys in **Secrets** (see above).
5. The app will rebuild and launch automatically.

---

## ğŸ“¦ Outputs

* **CSV**: Tabular file with metadata and PDF URLs.
* **JSONL**: JSON lines file for programmatic analysis.
* **ZIP**: All downloaded PDFs (optional) to your laptop.
* **failed\_downloads.csv**: Records with broken/missing PDFs.

---

## ğŸ§­ Usage Notes

* Files saved in the cloud environment are temporary. Use the **Download CSV** or **Download ZIP** buttons to save them locally.
* Respect API rate limits â€” use your keys and institutional email where required.
* The harvester does not bypass paywalls: PDF download succeeds only if a valid open-access link is available.
* Unpaywall email is set to `mohamed.aneddame-ext@um6p.ma` by default in the code (change if needed).

---

## ğŸ›  Troubleshooting

* **App build fails** â†’ verify `requirements.txt` contains every package you import.
* **Missing results from a source** â†’ ensure corresponding API key is set in secrets and the API quota isnâ€™t exhausted.
* **Downloads not appearing locally when deployed** â†’ the app stores files in the container; use the ZIP download button to save to your laptop.
* **OpenAlex/DOAJ errors** â†’ the code uses corrected endpoints and parameters; ensure youâ€™re running the latest `materials_papers_harvester.py`.

---

## ğŸ™ Attribution

This project queries and aggregates metadata from multiple scholarly APIs and respects each providerâ€™s terms of service:

* OpenAlex, Crossref, arXiv, Semantic Scholar, DOAJ, PubMed/NCBI E-utilities, Springer Metadata API, Elsevier/ScienceDirect API, IEEE Xplore.

---

## ğŸ“¬ Contact / Contributions

If you find bugs or want to contribute, please open an issue or pull request on the GitHub repo. For questions about API usage, include log output and the query you used.

---

*Happy harvesting!*
